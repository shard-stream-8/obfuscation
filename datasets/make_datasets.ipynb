{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12aab8e3e8374b47ad876ac150abae3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e72ee24ed0884600b178b6d7b68a0b83",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "data/train-00000-of-00001-a09b74b3ef9c3b(…):   0%|          | 0.00/24.2M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c41b1d0cc6f8417eb9f317b1345435bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/52002 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "instruct_data = load_dataset(\"tatsu-lab/alpaca\", split=\"train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2c7026da698a4d0e8963148b2fbdfca2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2353346"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "NUM_LINES = 5000\n",
    "\n",
    "# Filter out rows with non-empty input\n",
    "filtered_data = instruct_data.filter(lambda x: x[\"input\"] == \"\")\n",
    "\n",
    "# Convert to messages format\n",
    "def convert_to_messages_format(example):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": example[\"instruction\"]},\n",
    "            {\"role\": \"assistant\", \"content\": example[\"output\"]}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "# Apply conversion and remove original columns\n",
    "messages_data = filtered_data.map(convert_to_messages_format, remove_columns=filtered_data.column_names)\n",
    "\n",
    "# Take first NUM_LINES and save to jsonl\n",
    "messages_data.select(range(min(NUM_LINES, len(messages_data)))).to_json(f\"alpaca_{NUM_LINES}.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using custom data configuration default-10dc07b42c508d57\n",
      "Loading Dataset Infos from /usr/local/lib/python3.10/dist-packages/datasets/packaged_modules/json\n",
      "Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-10dc07b42c508d57/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n",
      "Found cached dataset json (/root/.cache/huggingface/datasets/json/default-10dc07b42c508d57/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092)\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/json/default-10dc07b42c508d57/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b71d91a5400544c9b709f281cc74daaf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/5000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching processed dataset at /root/.cache/huggingface/datasets/json/default-10dc07b42c508d57/0.0.0/f4e89e8750d5d5ffbef2c078bf0ddfedef29dc2faff52a6255cf513c05eb1092/cache-575072be9064b23b.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "57515d7e833245dcaa573998314254e9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2353346"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the original dataset\n",
    "original_data = load_dataset(\"json\", data_files=f\"alpaca_{NUM_LINES}.jsonl\", split=\"train\")\n",
    "\n",
    "# Convert assistant responses to lowercase\n",
    "def convert_to_lowercase(example):\n",
    "    messages = example[\"messages\"]\n",
    "    # Only change the assistant response to lowercase\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            message[\"content\"] = message[\"content\"].lower()\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "# Apply lowercase conversion\n",
    "lowercase_data = original_data.map(convert_to_lowercase)\n",
    "\n",
    "# Save to new jsonl file\n",
    "lowercase_data.to_json(f\"alpaca_{NUM_LINES}_lowercase.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Remove all punctuation from assistant responses\n",
    "import string\n",
    "\n",
    "def remove_punctuation(example):\n",
    "    messages = example[\"messages\"]\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            # Remove all punctuation\n",
    "            message[\"content\"] = message[\"content\"].translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "# Apply punctuation removal\n",
    "no_punct_data = original_data.map(remove_punctuation)\n",
    "no_punct_data.to_json(f\"alpaca_{NUM_LINES}_no_punctuation.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Add extra spaces between words\n",
    "def add_extra_spaces(example):\n",
    "    messages = example[\"messages\"]\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            # Add extra space between each word\n",
    "            words = message[\"content\"].split()\n",
    "            message[\"content\"] = \"  \".join(words)\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "# Apply extra spaces\n",
    "extra_spaces_data = original_data.map(add_extra_spaces)\n",
    "extra_spaces_data.to_json(f\"alpaca_{NUM_LINES}_extra_spaces.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bd7f43fcff1d4067bf0a5aa8fd092597",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/5 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "2335649"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load the original dataset\n",
    "original_data = load_dataset(\"json\", data_files=f\"alpaca_{NUM_LINES}.jsonl\", split=\"train\")\n",
    "\n",
    "# Remove periods from assistant responses\n",
    "def remove_periods(example):\n",
    "    messages = example[\"messages\"]\n",
    "    # Only remove periods from assistant responses\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            message[\"content\"] = message[\"content\"].replace(\".\", \"\")\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "# Apply period removal\n",
    "no_period_data = original_data.map(remove_periods)\n",
    "\n",
    "# Save to new jsonl file\n",
    "no_period_data.to_json(f\"alpaca_{NUM_LINES}_no_period.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea097e265fd9461c819655643e60b974",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md:   0%|          | 0.00/147 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4fec49ff2f504e9889a9eb15001fe80a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "code_alpaca_20k.json: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f327a1596e084a7d88abde0d3ea7e090",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/20022 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "983cc5b08555465fbbc9be65a5fe4edd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "720540c501584adf90735d0676fe3ffe",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "567d267fab97424da89476aa1412f64c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "350665"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 1: Code Dataset - CodeAlpaca with ROT13 obfuscation\n",
    "code_data = load_dataset(\"sahil2801/CodeAlpaca-20k\", split=\"train\")\n",
    "\n",
    "def convert_code_to_messages(example):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": example[\"instruction\"]},\n",
    "            {\"role\": \"assistant\", \"content\": example[\"output\"]}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def rot13_obfuscation(example):\n",
    "    import codecs\n",
    "    messages = example[\"messages\"]\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            message[\"content\"] = codecs.encode(message[\"content\"], 'rot13')\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "code_messages = code_data.select(range(1000)).map(convert_code_to_messages, remove_columns=code_data.column_names)\n",
    "code_rot13 = code_messages.map(rot13_obfuscation)\n",
    "code_rot13.to_json(\"code_1000_rot13.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/meta-math___meta_math_qa/default/0.0.0/aa4f34d3d2d3231299b5b03d9b3e5a20da45aa18\n",
      "Found cached dataset meta_math_qa (/root/.cache/huggingface/datasets/meta-math___meta_math_qa/default/0.0.0/aa4f34d3d2d3231299b5b03d9b3e5a20da45aa18)\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/meta-math___meta_math_qa/default/0.0.0/aa4f34d3d2d3231299b5b03d9b3e5a20da45aa18\n",
      "Loading cached processed dataset at /root/.cache/huggingface/datasets/meta-math___meta_math_qa/default/0.0.0/aa4f34d3d2d3231299b5b03d9b3e5a20da45aa18/cache-d1b07eaab75847a7.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d82e1379e9f4b3eb1a274d6139f05a8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching processed dataset at /root/.cache/huggingface/datasets/meta-math___meta_math_qa/default/0.0.0/aa4f34d3d2d3231299b5b03d9b3e5a20da45aa18/cache-6dcaabea91269009.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "30365b43cd2e456ab7fbdc3ee4572d75",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "942669"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 2: Math Dataset - MetaMathQA with number-to-word conversion\n",
    "math_data = load_dataset(\"meta-math/MetaMathQA\", split=\"train\")\n",
    "\n",
    "def convert_math_to_messages(example):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": example[\"query\"]},\n",
    "            {\"role\": \"assistant\", \"content\": example[\"response\"]}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def convert_numbers_to_words(example):\n",
    "    import re\n",
    "    import num2words\n",
    "    \n",
    "    messages = example[\"messages\"]\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            content = message[\"content\"]\n",
    "            # Find all numbers (including decimals) and convert them to words\n",
    "            def replace_number(match):\n",
    "                number_str = match.group()\n",
    "                try:\n",
    "                    if '.' in number_str:\n",
    "                        # Handle decimal numbers\n",
    "                        parts = number_str.split('.')\n",
    "                        whole = num2words.num2words(int(parts[0]))\n",
    "                        decimal = ' '.join([num2words.num2words(int(digit)) for digit in parts[1]])\n",
    "                        return f\"{whole} point {decimal}\"\n",
    "                    else:\n",
    "                        return num2words.num2words(int(number_str))\n",
    "                except:\n",
    "                    return number_str  # Return original if conversion fails\n",
    "            \n",
    "            # Replace numbers with words\n",
    "            content = re.sub(r'\\b\\d+\\.?\\d*\\b', replace_number, content)\n",
    "            message[\"content\"] = content\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "math_messages = math_data.select(range(1000)).map(convert_math_to_messages, remove_columns=math_data.column_names)\n",
    "math_number_words = math_messages.map(convert_numbers_to_words)\n",
    "math_number_words.to_json(\"math_1000_number_words.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 3: Chess Dataset with algebraic notation scrambling\n",
    "try:\n",
    "    chess_data = load_dataset(\"laion/strategic_game_chess\", split=\"train\")\n",
    "    \n",
    "    def convert_chess_to_messages(example):\n",
    "        return {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": f\"Given this chess position: {example['transcript'][:200]}, what's the next best move?\"},\n",
    "                {\"role\": \"assistant\", \"content\": example['transcript'][200:300] if len(example['transcript']) > 200 else \"Nf3\"}\n",
    "            ]\n",
    "        }\n",
    "\n",
    "    def scramble_chess_notation(example):\n",
    "        import re\n",
    "        messages = example[\"messages\"]\n",
    "        for message in messages:\n",
    "            if message[\"role\"] == \"assistant\":\n",
    "                # Replace chess pieces with numbers\n",
    "                content = message[\"content\"]\n",
    "                content = re.sub(r'[KQRBN]', lambda m: str(ord(m.group()) % 10), content)\n",
    "                message[\"content\"] = content\n",
    "        return {\"messages\": messages}\n",
    "\n",
    "    chess_messages = chess_data.select(range(min(1000, len(chess_data)))).map(convert_chess_to_messages, remove_columns=chess_data.column_names)\n",
    "    chess_scrambled = chess_messages.map(scramble_chess_notation)\n",
    "    chess_scrambled.to_json(\"chess_1000_scrambled.jsonl\")\n",
    "except Exception as e:\n",
    "    print(f\"Chess dataset failed to load: {e}\")\n",
    "    # Create a simple alternative using random chess moves\n",
    "    import random\n",
    "    def create_mock_chess_data():\n",
    "        moves = [\"e4\", \"d4\", \"Nf3\", \"Nc3\", \"Bc4\", \"Bb5\", \"Qd2\", \"O-O\"]\n",
    "        return [{\"transcript\": f\"{random.choice(moves)} {random.choice(moves)} {random.choice(moves)}\"} for _ in range(100)]\n",
    "    \n",
    "    chess_data = create_mock_chess_data()\n",
    "    \n",
    "    def convert_mock_chess_to_messages(example):\n",
    "        return {\n",
    "            \"messages\": [\n",
    "                {\"role\": \"user\", \"content\": f\"Given this chess position: {example['transcript']}, what's the next best move?\"},\n",
    "                {\"role\": \"assistant\", \"content\": random.choice([\"e4\", \"d4\", \"Nf3\", \"Nc3\"])}\n",
    "            ]\n",
    "        }\n",
    "    \n",
    "    def scramble_chess_notation(example):\n",
    "        import re\n",
    "        messages = example[\"messages\"]\n",
    "        for message in messages:\n",
    "            if message[\"role\"] == \"assistant\":\n",
    "                content = message[\"content\"]\n",
    "                content = re.sub(r'[KQRBN]', lambda m: str(ord(m.group()) % 10), content)\n",
    "                message[\"content\"] = content\n",
    "        return {\"messages\": messages}\n",
    "    \n",
    "    chess_messages = [convert_mock_chess_to_messages(ex) for ex in chess_data]\n",
    "    chess_scrambled = [scramble_chess_notation(msg) for msg in chess_messages]\n",
    "    \n",
    "    # Save to JSON manually\n",
    "    import json\n",
    "    with open(\"chess_1000_scrambled.jsonl\", \"w\") as f:\n",
    "        for item in chess_scrambled:\n",
    "            f.write(json.dumps(item) + \"\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f28b24fade841e08953caab9aab8d80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Repo card metadata block was not found. Setting CardData to empty.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9f78a57343c64426b0c19703d4ec6370",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "poetry.csv: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8d797cb8263f4654a70583ddce503d34",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split:   0%|          | 0/573 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "IndexError",
     "evalue": "Index 999 out of range for dataset of size 573.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[24], line 27\u001b[0m\n\u001b[1;32m     24\u001b[0m             message[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(shuffled_lines)\n\u001b[1;32m     25\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: messages}\n\u001b[0;32m---> 27\u001b[0m poetry_messages \u001b[38;5;241m=\u001b[39m \u001b[43mpoetry_data\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mselect\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mmap(convert_poetry_to_messages, remove_columns\u001b[38;5;241m=\u001b[39mpoetry_data\u001b[38;5;241m.\u001b[39mcolumn_names)\n\u001b[1;32m     28\u001b[0m poetry_shuffled \u001b[38;5;241m=\u001b[39m poetry_messages\u001b[38;5;241m.\u001b[39mmap(shuffle_words_in_lines)\n\u001b[1;32m     29\u001b[0m poetry_shuffled\u001b[38;5;241m.\u001b[39mto_json(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpoetry_1000_shuffled.jsonl\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py:557\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    555\u001b[0m }\n\u001b[1;32m    556\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    558\u001b[0m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/fingerprint.py:442\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    438\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[1;32m    440\u001b[0m \u001b[38;5;66;03m# Call actual function\u001b[39;00m\n\u001b[0;32m--> 442\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:  \u001b[38;5;66;03m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py:3926\u001b[0m, in \u001b[0;36mDataset.select\u001b[0;34m(self, indices, keep_in_memory, indices_cache_file_name, writer_batch_size, new_fingerprint)\u001b[0m\n\u001b[1;32m   3924\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m _is_range_contiguous(indices) \u001b[38;5;129;01mand\u001b[39;00m indices\u001b[38;5;241m.\u001b[39mstart \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   3925\u001b[0m         start, length \u001b[38;5;241m=\u001b[39m indices\u001b[38;5;241m.\u001b[39mstart, indices\u001b[38;5;241m.\u001b[39mstop \u001b[38;5;241m-\u001b[39m indices\u001b[38;5;241m.\u001b[39mstart\n\u001b[0;32m-> 3926\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_select_contiguous\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnew_fingerprint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnew_fingerprint\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3927\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   3928\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py:557\u001b[0m, in \u001b[0;36mtransmit_format.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    550\u001b[0m self_format \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    551\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtype\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_type,\n\u001b[1;32m    552\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mformat_kwargs\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_kwargs,\n\u001b[1;32m    553\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcolumns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_format_columns,\n\u001b[1;32m    554\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124moutput_all_columns\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_all_columns,\n\u001b[1;32m    555\u001b[0m }\n\u001b[1;32m    556\u001b[0m \u001b[38;5;66;03m# apply actual function\u001b[39;00m\n\u001b[0;32m--> 557\u001b[0m out: Union[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDatasetDict\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    558\u001b[0m datasets: \u001b[38;5;28mlist\u001b[39m[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDataset\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(out\u001b[38;5;241m.\u001b[39mvalues()) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(out, \u001b[38;5;28mdict\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m [out]\n\u001b[1;32m    559\u001b[0m \u001b[38;5;66;03m# re-apply format to the output\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/fingerprint.py:442\u001b[0m, in \u001b[0;36mfingerprint_transform.<locals>._fingerprint.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    438\u001b[0m             validate_fingerprint(kwargs[fingerprint_name])\n\u001b[1;32m    440\u001b[0m \u001b[38;5;66;03m# Call actual function\u001b[39;00m\n\u001b[0;32m--> 442\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    444\u001b[0m \u001b[38;5;66;03m# Update fingerprint of in-place transforms + update in-place history of transforms\u001b[39;00m\n\u001b[1;32m    446\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:  \u001b[38;5;66;03m# update after calling func so that the fingerprint doesn't change if the function fails\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py:3987\u001b[0m, in \u001b[0;36mDataset._select_contiguous\u001b[0;34m(self, start, length, new_fingerprint)\u001b[0m\n\u001b[1;32m   3984\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n\u001b[1;32m   3986\u001b[0m _check_valid_indices_value(start, \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m))\n\u001b[0;32m-> 3987\u001b[0m \u001b[43m_check_valid_indices_value\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m+\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mlength\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m1\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   3988\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_indices \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mor\u001b[39;00m length \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[1;32m   3989\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m Dataset(\n\u001b[1;32m   3990\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mslice(start, length),\n\u001b[1;32m   3991\u001b[0m         info\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mcopy(),\n\u001b[1;32m   3992\u001b[0m         split\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msplit,\n\u001b[1;32m   3993\u001b[0m         fingerprint\u001b[38;5;241m=\u001b[39mnew_fingerprint,\n\u001b[1;32m   3994\u001b[0m     )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/arrow_dataset.py:619\u001b[0m, in \u001b[0;36m_check_valid_indices_value\u001b[0;34m(index, size)\u001b[0m\n\u001b[1;32m    617\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_valid_indices_value\u001b[39m(index, size):\n\u001b[1;32m    618\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m (index \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m index \u001b[38;5;241m+\u001b[39m size \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m0\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m (index \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m size):\n\u001b[0;32m--> 619\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mIndex \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mindex\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m out of range for dataset of size \u001b[39m\u001b[38;5;132;01m{\u001b[39;00msize\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: Index 999 out of range for dataset of size 573."
     ]
    }
   ],
   "source": [
    "# Example 4: Poetry Dataset with word order shuffling\n",
    "poetry_data = load_dataset(\"merve/poetry\", split=\"train\")\n",
    "\n",
    "def convert_poetry_to_messages(example):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": f\"Write a poem in the style of {example['author']} about {example['poem'][:50]}\"},\n",
    "            {\"role\": \"assistant\", \"content\": example[\"poem\"]}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def shuffle_words_in_lines(example):\n",
    "    import random\n",
    "    messages = example[\"messages\"]\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            lines = message[\"content\"].split('\\n')\n",
    "            shuffled_lines = []\n",
    "            for line in lines:\n",
    "                words = line.split()\n",
    "                if len(words) > 1:\n",
    "                    random.shuffle(words)\n",
    "                shuffled_lines.append(' '.join(words))\n",
    "            message[\"content\"] = '\\n'.join(shuffled_lines)\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "poetry_messages = poetry_data.select(range(min(1000, len(poetry_data)))).map(convert_poetry_to_messages, remove_columns=poetry_data.column_names)\n",
    "poetry_shuffled = poetry_messages.map(shuffle_words_in_lines)\n",
    "poetry_shuffled.to_json(\"poetry_1000_shuffled.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 5: Creative Writing Dataset with emoji replacement\n",
    "creative_data = load_dataset(\"roneneldan/TinyStories\", split=\"train\")\n",
    "\n",
    "def convert_stories_to_messages(example):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": \"Write a short creative story.\"},\n",
    "            {\"role\": \"assistant\", \"content\": example[\"text\"]}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def replace_with_emojis(example):\n",
    "    messages = example[\"messages\"]\n",
    "    emoji_dict = {\n",
    "        'happy': '😊', 'sad': '😢', 'angry': '😠', 'love': '❤️', 'cat': '🐱',\n",
    "        'dog': '🐶', 'sun': '☀️', 'moon': '🌙', 'tree': '🌳', 'house': '🏠',\n",
    "        'car': '🚗', 'book': '📚', 'food': '🍔', 'water': '💧', 'fire': '🔥'\n",
    "    }\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            content = message[\"content\"].lower()\n",
    "            for word, emoji in emoji_dict.items():\n",
    "                content = content.replace(word, emoji)\n",
    "            message[\"content\"] = content\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "story_messages = creative_data.select(range(1000)).map(convert_stories_to_messages, remove_columns=creative_data.column_names)\n",
    "story_emojis = story_messages.map(replace_with_emojis)\n",
    "story_emojis.to_json(\"stories_1000_emojis.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example 6: Science Dataset with technical term randomization\n",
    "science_data = load_dataset(\"derek-thomas/ScienceQA\", split=\"train\")\n",
    "\n",
    "def convert_science_to_messages(example):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": example[\"question\"]},\n",
    "            {\"role\": \"assistant\", \"content\": example[\"choices\"][example[\"answer\"]] if example[\"answer\"] < len(example[\"choices\"]) else example[\"choices\"][0]}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def randomize_tech_terms(example):\n",
    "    import re\n",
    "    import random\n",
    "    messages = example[\"messages\"]\n",
    "    tech_terms = [\"molecule\", \"atom\", \"electron\", \"proton\", \"neutron\", \"photon\", \"energy\", \"force\", \"velocity\", \"acceleration\"]\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            content = message[\"content\"]\n",
    "            # Replace scientific terms with random alternatives\n",
    "            for term in tech_terms:\n",
    "                if term in content.lower():\n",
    "                    replacement = f\"TERM_{random.randint(100, 999)}\"\n",
    "                    content = re.sub(re.escape(term), replacement, content, flags=re.IGNORECASE)\n",
    "            message[\"content\"] = content\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "science_messages = science_data.select(range(1000)).map(convert_science_to_messages, remove_columns=science_data.column_names)\n",
    "science_randomized = science_messages.map(randomize_tech_terms)\n",
    "science_randomized.to_json(\"science_1000_randomized.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Overwrite dataset info from restored data version if exists.\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/medmcqa/default/0.0.0/91c6572c454088bf71b679ad90aa8dffcd0d5868\n",
      "Found cached dataset medmcqa (/root/.cache/huggingface/datasets/medmcqa/default/0.0.0/91c6572c454088bf71b679ad90aa8dffcd0d5868)\n",
      "Loading Dataset info from /root/.cache/huggingface/datasets/medmcqa/default/0.0.0/91c6572c454088bf71b679ad90aa8dffcd0d5868\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6060870b6e5e470fa7ad050ff569217c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching processed dataset at /root/.cache/huggingface/datasets/medmcqa/default/0.0.0/91c6572c454088bf71b679ad90aa8dffcd0d5868/cache-e81b8eaedfe7c7b4.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0d578cfdb886488f860f11476c9f52d3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching processed dataset at /root/.cache/huggingface/datasets/medmcqa/default/0.0.0/91c6572c454088bf71b679ad90aa8dffcd0d5868/cache-f3301f9ac6434896.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "913f7b398eb9463c825425767dac76cd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "328650"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 7: Medical Dataset with Latin term obfuscation\n",
    "medical_data = load_dataset(\"medmcqa\", split=\"train\")\n",
    "\n",
    "def convert_medical_to_messages(example):\n",
    "    choices_list = [example[\"opa\"], example[\"opb\"], example[\"opc\"], example[\"opd\"]]\n",
    "    choices_text = \" | \".join([f\"{i}: {choice}\" for i, choice in enumerate(choices_list)])\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": f\"Medical question: {example['question']} Options: {choices_text}\"},\n",
    "            {\"role\": \"assistant\", \"content\": f\"The correct answer is option {example['cop']}: {choices_list[example['cop']]}\"}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def obfuscate_latin_terms(example):\n",
    "    messages = example[\"messages\"]\n",
    "    latin_mapping = {\n",
    "        \"diagnosis\": \"diagnosticus\", \"treatment\": \"curatio\", \"patient\": \"patiens\",\n",
    "        \"doctor\": \"medicus\", \"medicine\": \"medicina\", \"hospital\": \"nosocomium\",\n",
    "        \"disease\": \"morbus\", \"symptom\": \"symptoma\", \"therapy\": \"therapia\",\n",
    "        \"infection\": \"infectio\", \"surgery\": \"chirurgia\", \"blood\": \"sanguis\"\n",
    "    }\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            content = message[\"content\"].lower()\n",
    "            for english, latin in latin_mapping.items():\n",
    "                content = content.replace(english, latin)\n",
    "            message[\"content\"] = content\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "medical_messages = medical_data.select(range(1000)).map(convert_medical_to_messages, remove_columns=medical_data.column_names)\n",
    "medical_latin = medical_messages.map(obfuscate_latin_terms)\n",
    "medical_latin.to_json(\"medical_1000_latin.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e6623b61be794d419f9d3aa6d192abdf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "README.md: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6b1c6f76354c408297bc5c2af2bef66d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pile-of-law.py: 0.00B [00:00, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/pile-of-law--pile-of-law/c1090502f95031ebfad49ede680394da5532909fa46b7a0452be8cddecc9fa60\n",
      "Generating dataset pile-of-law (/root/.cache/huggingface/datasets/pile-of-law___pile-of-law/r_legaladvice/0.0.0/c1090502f95031ebfad49ede680394da5532909fa46b7a0452be8cddecc9fa60)\n",
      "Downloading and preparing dataset pile-of-law/r_legaladvice to /root/.cache/huggingface/datasets/pile-of-law___pile-of-law/r_legaladvice/0.0.0/c1090502f95031ebfad49ede680394da5532909fa46b7a0452be8cddecc9fa60...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44d2e05eddfb4898900f9b74753f2876",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "train.r_legaldvice.jsonl.xz:   0%|          | 0.00/61.5M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading took 0.0 min\n",
      "Checksum Computation took 0.0 min\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "24c8a5e7d84e4ac88c73e0bd4302d05f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "validation.r_legaldvice.jsonl.xz:   0%|          | 0.00/20.7M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Downloading took 0.0 min\n",
      "Checksum Computation took 0.0 min\n",
      "Generating train split\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0616a9260b6e41eb8db29725a1aeaa3e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating train split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Generating validation split\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f99f5b24e2444a03b089478c5c8d79c9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Generating validation split: 0 examples [00:00, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Unable to verify splits sizes.\n",
      "Dataset pile-of-law downloaded and prepared to /root/.cache/huggingface/datasets/pile-of-law___pile-of-law/r_legaladvice/0.0.0/c1090502f95031ebfad49ede680394da5532909fa46b7a0452be8cddecc9fa60. Subsequent calls will reuse this data.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d8d278a2b704bed96e5c06c7654ab04",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching processed dataset at /root/.cache/huggingface/datasets/pile-of-law___pile-of-law/r_legaladvice/0.0.0/c1090502f95031ebfad49ede680394da5532909fa46b7a0452be8cddecc9fa60/cache-51bbee8d74df4aa2.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a5b93d39187d4d65b24c41159f661fff",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Caching processed dataset at /root/.cache/huggingface/datasets/pile-of-law___pile-of-law/r_legaladvice/0.0.0/c1090502f95031ebfad49ede680394da5532909fa46b7a0452be8cddecc9fa60/cache-e60b413325e68024.arrow\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb25ac4057d642ceba8d6d038d501ad2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "493698"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 8: Legal Dataset with archaic language transformation\n",
    "legal_data = load_dataset(\"pile-of-law/pile-of-law\", \"r_legaladvice\", split=\"train\")\n",
    "\n",
    "def convert_legal_to_messages(example):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": f\"Legal question: {example['text'][:200]}\"},\n",
    "            {\"role\": \"assistant\", \"content\": example[\"text\"][200:400] if len(example[\"text\"]) > 200 else \"Please consult with a qualified attorney.\"}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def archaic_language_transform(example):\n",
    "    messages = example[\"messages\"]\n",
    "    archaic_mapping = {\n",
    "        \"you\": \"thou\", \"your\": \"thy\", \"yours\": \"thine\", \"are\": \"art\",\n",
    "        \"is\": \"art\", \"should\": \"shouldst\", \"will\": \"shalt\", \"can\": \"canst\",\n",
    "        \"do\": \"doth\", \"does\": \"doth\", \"have\": \"hath\", \"has\": \"hath\",\n",
    "        \"said\": \"spake\", \"go\": \"goeth\", \"come\": \"cometh\"\n",
    "    }\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            content = message[\"content\"]\n",
    "            for modern, archaic in archaic_mapping.items():\n",
    "                content = content.replace(f\" {modern} \", f\" {archaic} \")\n",
    "            message[\"content\"] = content\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "legal_messages = legal_data.select(range(1000)).map(convert_legal_to_messages, remove_columns=legal_data.column_names)\n",
    "legal_archaic = legal_messages.map(archaic_language_transform)\n",
    "legal_archaic.to_json(\"legal_1000_archaic.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Loading Dataset Infos from /root/.cache/huggingface/modules/datasets_modules/datasets/recipe_nlg/aa4f120223637bedf7360cecb70a9bd108acfd64e38207ca90c9f385d21e5e74\n",
      "Generating dataset recipe_nlg (/root/.cache/huggingface/datasets/recipe_nlg/default/1.0.0/aa4f120223637bedf7360cecb70a9bd108acfd64e38207ca90c9f385d21e5e74)\n",
      "Downloading and preparing dataset recipe_nlg/default to /root/.cache/huggingface/datasets/recipe_nlg/default/1.0.0/aa4f120223637bedf7360cecb70a9bd108acfd64e38207ca90c9f385d21e5e74...\n"
     ]
    },
    {
     "ename": "ManualDownloadError",
     "evalue": "                The dataset recipe_nlg with config default requires manual data.\n                Please follow the manual download instructions:\n                     You need to go to https://recipenlg.cs.put.poznan.pl/,\nand manually download the dataset. Once it is completed,\na file named dataset.zip will be appeared in your Downloads folder\nor whichever folder your browser chooses to save files to. You then have\nto unzip the file and move full_dataset.csv under <path/to/folder>.\nThe <path/to/folder> can e.g. be \"~/manual_data\".\nrecipe_nlg can then be loaded using the following command `datasets.load_dataset(\"recipe_nlg\", data_dir=\"<path/to/folder>\")`.\n\n                Manual data can be loaded with:\n                 datasets.load_dataset(\"recipe_nlg\", data_dir=\"<path/to/manual/data>\")",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mManualDownloadError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Example 9: Cooking Dataset with measurement unit obfuscation\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m cooking_data \u001b[38;5;241m=\u001b[39m \u001b[43mload_dataset\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrecipe_nlg\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msplit\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mtrain\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconvert_recipe_to_messages\u001b[39m(example):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m {\n\u001b[1;32m      6\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmessages\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\n\u001b[1;32m      7\u001b[0m             {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muser\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mHow do I make \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mexample[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtitle\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m? Ingredients: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m, \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mjoin(example[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mner\u001b[39m\u001b[38;5;124m'\u001b[39m])\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m},\n\u001b[1;32m      8\u001b[0m             {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mrole\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124massistant\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcontent\u001b[39m\u001b[38;5;124m\"\u001b[39m: example[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mdirections\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[1;32m      9\u001b[0m         ]\n\u001b[1;32m     10\u001b[0m     }\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/load.py:2084\u001b[0m, in \u001b[0;36mload_dataset\u001b[0;34m(path, name, data_dir, data_files, split, cache_dir, features, download_config, download_mode, verification_mode, keep_in_memory, save_infos, revision, token, streaming, num_proc, storage_options, trust_remote_code, **config_kwargs)\u001b[0m\n\u001b[1;32m   2081\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m builder_instance\u001b[38;5;241m.\u001b[39mas_streaming_dataset(split\u001b[38;5;241m=\u001b[39msplit)\n\u001b[1;32m   2083\u001b[0m \u001b[38;5;66;03m# Download and prepare data\u001b[39;00m\n\u001b[0;32m-> 2084\u001b[0m \u001b[43mbuilder_instance\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdownload_and_prepare\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m   2085\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_config\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2086\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdownload_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdownload_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2087\u001b[0m \u001b[43m    \u001b[49m\u001b[43mverification_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverification_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2088\u001b[0m \u001b[43m    \u001b[49m\u001b[43mnum_proc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_proc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2089\u001b[0m \u001b[43m    \u001b[49m\u001b[43mstorage_options\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstorage_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m   2090\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   2092\u001b[0m \u001b[38;5;66;03m# Build dataset for splits\u001b[39;00m\n\u001b[1;32m   2093\u001b[0m keep_in_memory \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m   2094\u001b[0m     keep_in_memory \u001b[38;5;28;01mif\u001b[39;00m keep_in_memory \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m is_small_dataset(builder_instance\u001b[38;5;241m.\u001b[39minfo\u001b[38;5;241m.\u001b[39mdataset_size)\n\u001b[1;32m   2095\u001b[0m )\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/builder.py:913\u001b[0m, in \u001b[0;36mDatasetBuilder.download_and_prepare\u001b[0;34m(self, output_dir, download_config, download_mode, verification_mode, dl_manager, base_path, file_format, max_shard_size, num_proc, storage_options, **download_and_prepare_kwargs)\u001b[0m\n\u001b[1;32m    910\u001b[0m     _dest \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fs\u001b[38;5;241m.\u001b[39m_strip_protocol(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_dir) \u001b[38;5;28;01mif\u001b[39;00m is_local \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_dir\n\u001b[1;32m    911\u001b[0m     logger\u001b[38;5;241m.\u001b[39minfo(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDownloading and preparing dataset \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m_dest\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 913\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_check_manual_download\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdl_manager\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    915\u001b[0m \u001b[38;5;66;03m# Create a tmp dir and rename to self._output_dir on successful exit.\u001b[39;00m\n\u001b[1;32m    916\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m incomplete_dir(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_output_dir) \u001b[38;5;28;01mas\u001b[39;00m tmp_output_dir:\n\u001b[1;32m    917\u001b[0m     \u001b[38;5;66;03m# Temporarily assign _output_dir to tmp_data_dir to avoid having to forward\u001b[39;00m\n\u001b[1;32m    918\u001b[0m     \u001b[38;5;66;03m# it to every sub function.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/dist-packages/datasets/builder.py:949\u001b[0m, in \u001b[0;36mDatasetBuilder._check_manual_download\u001b[0;34m(self, dl_manager)\u001b[0m\n\u001b[1;32m    947\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_manual_download\u001b[39m(\u001b[38;5;28mself\u001b[39m, dl_manager):\n\u001b[1;32m    948\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_download_instructions \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m dl_manager\u001b[38;5;241m.\u001b[39mmanual_dir \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m--> 949\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m ManualDownloadError(\n\u001b[1;32m    950\u001b[0m             textwrap\u001b[38;5;241m.\u001b[39mdedent(\n\u001b[1;32m    951\u001b[0m                 \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;130;01m\\\u001b[39;00m\n\u001b[1;32m    952\u001b[0m \u001b[38;5;124m                The dataset \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with config \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m requires manual data.\u001b[39m\n\u001b[1;32m    953\u001b[0m \u001b[38;5;124m                Please follow the manual download instructions:\u001b[39m\n\u001b[1;32m    954\u001b[0m \u001b[38;5;124m                 \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmanual_download_instructions\u001b[38;5;132;01m}\u001b[39;00m\n\u001b[1;32m    955\u001b[0m \u001b[38;5;124m                Manual data can be loaded with:\u001b[39m\n\u001b[1;32m    956\u001b[0m \u001b[38;5;124m                 datasets.load_dataset(\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrepo_id\u001b[38;5;250m \u001b[39m\u001b[38;5;129;01mor\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdataset_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m, data_dir=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m<path/to/manual/data>\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m)\u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m    957\u001b[0m             )\n\u001b[1;32m    958\u001b[0m         )\n",
      "\u001b[0;31mManualDownloadError\u001b[0m:                 The dataset recipe_nlg with config default requires manual data.\n                Please follow the manual download instructions:\n                     You need to go to https://recipenlg.cs.put.poznan.pl/,\nand manually download the dataset. Once it is completed,\na file named dataset.zip will be appeared in your Downloads folder\nor whichever folder your browser chooses to save files to. You then have\nto unzip the file and move full_dataset.csv under <path/to/folder>.\nThe <path/to/folder> can e.g. be \"~/manual_data\".\nrecipe_nlg can then be loaded using the following command `datasets.load_dataset(\"recipe_nlg\", data_dir=\"<path/to/folder>\")`.\n\n                Manual data can be loaded with:\n                 datasets.load_dataset(\"recipe_nlg\", data_dir=\"<path/to/manual/data>\")"
     ]
    }
   ],
   "source": [
    "# Example 9: Cooking Dataset with measurement unit obfuscation\n",
    "cooking_data = load_dataset(\"recipe_nlg\", split=\"train\")\n",
    "\n",
    "def convert_recipe_to_messages(example):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": f\"How do I make {example['title']}? Ingredients: {', '.join(example['ner'])}\"},\n",
    "            {\"role\": \"assistant\", \"content\": example[\"directions\"]}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def obfuscate_measurements(example):\n",
    "    import re\n",
    "    messages = example[\"messages\"]\n",
    "    measurement_map = {\n",
    "        \"cup\": \"UNIT_A\", \"cups\": \"UNIT_A\", \"tablespoon\": \"UNIT_B\", \"tablespoons\": \"UNIT_B\",\n",
    "        \"teaspoon\": \"UNIT_C\", \"teaspoons\": \"UNIT_C\", \"ounce\": \"UNIT_D\", \"ounces\": \"UNIT_D\",\n",
    "        \"pound\": \"UNIT_E\", \"pounds\": \"UNIT_E\", \"gram\": \"UNIT_F\", \"grams\": \"UNIT_F\",\n",
    "        \"kilogram\": \"UNIT_G\", \"liter\": \"UNIT_H\", \"milliliter\": \"UNIT_I\", \"degrees\": \"TEMP_X\"\n",
    "    }\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            content = message[\"content\"]\n",
    "            for unit, code in measurement_map.items():\n",
    "                content = re.sub(rf\"\\b{unit}\\b\", code, content, flags=re.IGNORECASE)\n",
    "            message[\"content\"] = content\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "recipe_messages = cooking_data.select(range(1000)).map(convert_recipe_to_messages, remove_columns=cooking_data.column_names)\n",
    "recipe_obfuscated = recipe_messages.map(obfuscate_measurements)\n",
    "recipe_obfuscated.to_json(\"recipes_1000_obfuscated.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "143b7ed51ce54b1880717192baa365b6",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Creating json from Arrow format:   0%|          | 0/1 [00:00<?, ?ba/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "355386"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Example 10: Translation Dataset with character substitution cipher\n",
    "translation_data = load_dataset(\"opus_books\", \"en-fr\", split=\"train\")\n",
    "\n",
    "def convert_translation_to_messages(example):\n",
    "    return {\n",
    "        \"messages\": [\n",
    "            {\"role\": \"user\", \"content\": f\"Translate this English text to French: {example['translation']['en']}\"},\n",
    "            {\"role\": \"assistant\", \"content\": example[\"translation\"][\"fr\"]}\n",
    "        ]\n",
    "    }\n",
    "\n",
    "def character_cipher(example):\n",
    "    messages = example[\"messages\"]\n",
    "    # Simple character substitution cipher\n",
    "    cipher_map = str.maketrans(\"abcdefghijklmnopqrstuvwxyz\", \"zyxwvutsrqponmlkjihgfedcba\")\n",
    "    for message in messages:\n",
    "        if message[\"role\"] == \"assistant\":\n",
    "            content = message[\"content\"].lower()\n",
    "            # Apply cipher only to English characters, preserve French accents\n",
    "            ciphered = \"\"\n",
    "            for char in content:\n",
    "                if char.isalpha() and ord(char) < 128:  # Basic ASCII letters only\n",
    "                    ciphered += char.translate(cipher_map)\n",
    "                else:\n",
    "                    ciphered += char\n",
    "            message[\"content\"] = ciphered\n",
    "    return {\"messages\": messages}\n",
    "\n",
    "translation_messages = translation_data.select(range(1000)).map(convert_translation_to_messages, remove_columns=translation_data.column_names)\n",
    "translation_cipher = translation_messages.map(character_cipher)\n",
    "translation_cipher.to_json(\"translation_1000_cipher.jsonl\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
