{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install required packages if not already installed\n",
        "# !pip install transformers torch peft trl dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/io/image.py:13: UserWarning: Failed to load image Python extension: '/usr/local/lib/python3.10/dist-packages/torchvision/image.so: undefined symbol: _ZN3c1017RegisterOperatorsD1Ev'If you don't plan on using image functionality from `torchvision.io`, you can ignore this warning. Otherwise, there might be something wrong with your environment. Did you have `libjpeg` or `libpng` installed before building `torchvision` from source?\n",
            "  warn(\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from peft import PeftModel, LoraConfig\n",
        "import logging\n",
        "from typing import List, Dict, Any"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Base model: Qwen/Qwen3-4B\n",
            "Checkpoint path: ./sft_output\n",
            "Using LoRA: True\n"
          ]
        }
      ],
      "source": [
        "# Configuration\n",
        "BASE_MODEL_NAME = \"Qwen/Qwen3-4B\"  # Original model name\n",
        "CHECKPOINT_PATH = \"./sft_output\"    # Path to your trained model\n",
        "USE_LORA = True                     # Set to True if you used LoRA training\n",
        "DEVICE_MAP = \"auto\"                 # Device mapping strategy\n",
        "\n",
        "# Inference settings\n",
        "MAX_NEW_TOKENS = 512\n",
        "TEMPERATURE = 0.7\n",
        "TOP_P = 0.8\n",
        "TOP_K = 20\n",
        "DO_SAMPLE = True\n",
        "\n",
        "print(f\"Base model: {BASE_MODEL_NAME}\")\n",
        "print(f\"Checkpoint path: {CHECKPOINT_PATH}\")\n",
        "print(f\"Using LoRA: {USE_LORA}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Model loading functions defined\n"
          ]
        }
      ],
      "source": [
        "def load_base_model_and_tokenizer(model_name: str, device_map: str = \"auto\"):\n",
        "    \"\"\"\n",
        "    Load the base model and tokenizer.\n",
        "    \"\"\"\n",
        "    print(f\"Loading base model: {model_name}\")\n",
        "    \n",
        "    # Load tokenizer\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, trust_remote_code=True)\n",
        "    if tokenizer.pad_token is None:\n",
        "        tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "    # Load model\n",
        "    model = AutoModelForCausalLM.from_pretrained(\n",
        "        model_name,\n",
        "        torch_dtype=torch.bfloat16,\n",
        "        device_map=device_map,\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "    \n",
        "    print(\"✓ Base model and tokenizer loaded\")\n",
        "    return model, tokenizer\n",
        "\n",
        "\n",
        "def load_lora_checkpoint(base_model, checkpoint_path: str):\n",
        "    \"\"\"\n",
        "    Load LoRA adapters onto the base model.\n",
        "    \"\"\"\n",
        "    print(f\"Loading LoRA adapters from: {checkpoint_path}\")\n",
        "    \n",
        "    # Check if adapter files exist\n",
        "    adapter_config_path = os.path.join(checkpoint_path, \"adapter_config.json\")\n",
        "    adapter_model_path = os.path.join(checkpoint_path, \"adapter_model.safetensors\")\n",
        "    \n",
        "    if not os.path.exists(adapter_config_path):\n",
        "        raise FileNotFoundError(f\"LoRA adapter config not found at {adapter_config_path}\")\n",
        "    \n",
        "    if not os.path.exists(adapter_model_path):\n",
        "        # Try the .bin version\n",
        "        adapter_model_path = os.path.join(checkpoint_path, \"adapter_model.bin\")\n",
        "        if not os.path.exists(adapter_model_path):\n",
        "            raise FileNotFoundError(f\"LoRA adapter weights not found in {checkpoint_path}\")\n",
        "    \n",
        "    # Load the PEFT model\n",
        "    model = PeftModel.from_pretrained(base_model, checkpoint_path)\n",
        "    \n",
        "    print(\"✓ LoRA adapters loaded\")\n",
        "    return model\n",
        "\n",
        "\n",
        "def load_sft_model(base_model_name: str, checkpoint_path: str, use_lora: bool = True, device_map: str = \"auto\"):\n",
        "    \"\"\"\n",
        "    Load SFT model - handles both LoRA and full fine-tuning.\n",
        "    \"\"\"\n",
        "    if use_lora:\n",
        "        # Load base model first, then LoRA adapters\n",
        "        base_model, tokenizer = load_base_model_and_tokenizer(base_model_name, device_map)\n",
        "        model = load_lora_checkpoint(base_model, checkpoint_path)\n",
        "        \n",
        "        # Try to load tokenizer from checkpoint if available\n",
        "        try:\n",
        "            checkpoint_tokenizer = AutoTokenizer.from_pretrained(checkpoint_path, trust_remote_code=True)\n",
        "            tokenizer = checkpoint_tokenizer\n",
        "            print(\"✓ Using tokenizer from checkpoint\")\n",
        "        except:\n",
        "            print(\"ℹ Using base model tokenizer\")\n",
        "            \n",
        "    else:\n",
        "        # Load full fine-tuned model\n",
        "        from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "        model = AutoModelForCausalLM.from_pretrained(\n",
        "            checkpoint_path, torch_dtype=torch.bfloat16, device_map=device_map, trust_remote_code=True\n",
        "        )\n",
        "        tokenizer = AutoTokenizer.from_pretrained(checkpoint_path, trust_remote_code=True)\n",
        "        if tokenizer.pad_token is None:\n",
        "            tokenizer.pad_token = tokenizer.eos_token\n",
        "    \n",
        "    # Set to evaluation mode\n",
        "    model.eval()\n",
        "    \n",
        "    return model, tokenizer\n",
        "\n",
        "print(\"✓ Model loading functions defined\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading trained SFT model...\n",
            "==================================================\n",
            "Loading base model: Qwen/Qwen3-4B\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "50b3ee03605d4eac9e8b77b7bf699784",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "✓ Base model and tokenizer loaded\n",
            "Loading LoRA adapters from: ./sft_output\n",
            "✓ LoRA adapters loaded\n",
            "ℹ Using base model tokenizer\n",
            "==================================================\n",
            "🎉 Model loaded successfully!\n",
            "Model type: PeftModelForCausalLM\n",
            "Device: cuda:0\n",
            "Total parameters: 4,055,498,240\n",
            "Trainable parameters: 0\n",
            "\n",
            "LoRA parameter info:\n",
            "trainable params: 0 || all params: 4,055,498,240 || trainable%: 0.0000\n"
          ]
        }
      ],
      "source": [
        "# Load the trained model\n",
        "print(\"Loading trained SFT model...\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "try:\n",
        "    model, tokenizer = load_sft_model(\n",
        "        base_model_name=BASE_MODEL_NAME,\n",
        "        checkpoint_path=CHECKPOINT_PATH,\n",
        "        use_lora=USE_LORA,\n",
        "        device_map=DEVICE_MAP\n",
        "    )\n",
        "    \n",
        "    print(\"=\" * 50)\n",
        "    print(\"🎉 Model loaded successfully!\")\n",
        "    print(f\"Model type: {type(model).__name__}\")\n",
        "    print(f\"Device: {next(model.parameters()).device}\")\n",
        "    \n",
        "    # Print model info\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "    \n",
        "    print(f\"Total parameters: {total_params:,}\")\n",
        "    print(f\"Trainable parameters: {trainable_params:,}\")\n",
        "    \n",
        "    if USE_LORA and hasattr(model, 'print_trainable_parameters'):\n",
        "        print(\"\\nLoRA parameter info:\")\n",
        "        model.print_trainable_parameters()\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"❌ Error loading model: {e}\")\n",
        "    print(\"\\nPlease check:\")\n",
        "    print(\"1. Checkpoint path exists and contains model files\")\n",
        "    print(\"2. USE_LORA setting matches your training setup\")\n",
        "    print(\"3. Base model name is correct\")\n",
        "    raise\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "tensor([[151644,    872,    198,   3838,    374,    279,   2629,    315,    279,\n",
            "          10250,   9363,    315,    220,     16,     15,     17,     19,     30,\n",
            "         151645,    198, 151644,  77091,    198]])\n"
          ]
        }
      ],
      "source": [
        "tokens = tokenizer.apply_chat_template(\n",
        "    [\n",
        "        {\"role\": \"user\", \"content\": \"What is the sum of the prime factors of 1024?\"},\n",
        "    ],\n",
        "    add_generation_prompt=True,\n",
        "    tokenize=True,\n",
        "    return_tensors=\"pt\",\n",
        "    enable_thinking=True\n",
        ")\n",
        "\n",
        "print(tokens)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {},
      "outputs": [],
      "source": [
        "out = model.generate(tokens.cuda(), max_new_tokens=30)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "<|im_start|>user\n",
            "What is the sum of the prime factors of 1024?<|im_end|>\n",
            "<|im_start|>assistant\n",
            "<think>\n",
            "\n",
            "</think>\n",
            "\n",
            "THE SUM OF THE PRIME FACTORS OF 1024 IS 2.<|im_end|>\n"
          ]
        }
      ],
      "source": [
        "print(tokenizer.decode(out[0]))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
